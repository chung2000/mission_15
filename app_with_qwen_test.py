import streamlit as st
from transformers import pipeline, Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
import torch
import time
import os
import sys

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from app_header import sidebar_menu, model_dict

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Advanced Vision AI", layout="centered")

# 2. ì‚¬ì´ë“œë°” ë©”ë‰´ (ëª¨ë¸ëª…ì— 'Qwen'ì´ í¬í•¨ëœ ê²½ìš°ë¥¼ ëŒ€ë¹„)
# model_dictì— "Qwen2.5-VL-3B": "Qwen/Qwen2.5-VL-3B-Instruct" ë“±ì„ ì¶”ê°€í•´ë‘ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
selected_name, top_k_count = sidebar_menu()
model_id = model_dict[selected_name]


# 3. ëª¨ë¸ ë¡œë“œ ë¡œì§ (Qwenê³¼ ì¼ë°˜ ëª¨ë¸ ë¶„ë¦¬)
@st.cache_resource
def load_ai_model(m_id):
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Qwen ê³„ì—´ ëª¨ë¸ì¸ ê²½ìš°
    if "qwen" in m_id.lower():
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            m_id, torch_dtype="auto", device_map="auto"
        )
        processor = AutoProcessor.from_pretrained(m_id)
        return {"model": model, "processor": processor, "type": "qwen"}

    # ì¼ë°˜ ë¶„ë¥˜ ëª¨ë¸ì¸ ê²½ìš°
    else:
        pipe = pipeline("image-classification", model=m_id, device=0 if device == "cuda" else -1)
        return {"model": pipe, "type": "classification"}


with st.spinner(f"[{selected_name}] ëª¨ë¸ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
    engine = load_ai_model(model_id)

# 4. ë©”ì¸ UI
st.title("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ë¹„ì „ ë¶„ì„ê¸°")
uploaded_file = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['jpg', 'jpeg', 'png', 'jfif', 'webp'])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")

    # ì§€ì‹œì‚¬í•­ ë°˜ì˜: width='stretch'
    st.image(image, caption="ë¶„ì„ ì´ë¯¸ì§€", width='stretch')

    alert_placeholder = st.empty()

    # 5. ëª¨ë¸ íƒ€ì…ë³„ ì¶”ë¡  í”„ë¡œì„¸ìŠ¤
    if engine["type"] == "classification":
        with st.spinner("ì´ë¯¸ì§€ ë¶„ë¥˜ ì¤‘..."):
            results = engine["model"](image)
            alert_placeholder.success("âœ… ë¶„ë¥˜ ì™„ë£Œ!")

            st.subheader("ğŸ“Š ë¶„ë¥˜ ê²°ê³¼")
            for res in results[:top_k_count]:
                col1, col2 = st.columns([1, 4])
                with col1: st.write(f"**{res['label']}**")
                with col2: st.progress(res['score']); st.write(f"{round(res['score'] * 100, 2)}%")

    elif engine["type"] == "qwen":
        # Qwen ëª¨ë“œì—ì„œëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        user_prompt = st.text_input("AIì—ê²Œ ì´ë¯¸ì§€ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”", value="Describe this image in detail.")

        if st.button("ì§ˆë¬¸í•˜ê¸°"):
            with st.spinner("Qwenì´ ì´ë¯¸ì§€ë¥¼ í•´ì„ ì¤‘..."):
                model = engine["model"]
                processor = engine["processor"]

                # Qwen ì „ìš© ì…ë ¥ êµ¬ì„±
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": image},
                            {"type": "text", "text": user_prompt},
                        ],
                    }
                ]

                # í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ í”„ë¡œì„¸ì‹±
                text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                image_inputs, _ = process_vision_info(messages)
                inputs = processor(
                    text=[text],
                    images=image_inputs,
                    padding=True,
                    return_tensors="pt"
                ).to(model.device)

                # ë‹µë³€ ìƒì„±
                generated_ids = model.generate(**inputs, max_new_tokens=256)
                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                output_text = processor.batch_decode(
                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                )

                alert_placeholder.success("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
                st.subheader("ğŸ’¬ AI ë‹µë³€")
                st.info(output_text[0])

    # ì•Œë¦¼ ìë™ ì‚­ì œ
    time.sleep(3)
    alert_placeholder.empty()